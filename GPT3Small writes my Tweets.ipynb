{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPT3Small writes my Tweets.ipynb","private_outputs":true,"provenance":[{"file_id":"1bwNxmVJMJ3x_N5ylS-nylkQpHUAF0DES","timestamp":1606241802968},{"file_id":"1asD3mi9SdygVNWCE94bIPpciTrbbPIWd","timestamp":1605862234662},{"file_id":"1h6r6Qg9xwyIzz6-FXgB9tIjAzce0gc2d","timestamp":1605190861790},{"file_id":"https://github.com/sberbank-ai/ru-gpts/blob/master/examples/Finetune_ruGPT3Small.ipynb","timestamp":1604407812564}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3RHDK81QqrET"},"source":["# Finetune ruGPT3Small on your data\n"]},{"cell_type":"markdown","metadata":{"id":"pO8Sv7ZgiVFT"},"source":["Tutorial and explications are here: https://habr.com/ru/company/sberbank/blog/528966/"]},{"cell_type":"markdown","metadata":{"id":"iTcPUwKZVS8-"},"source":["First, connect Google Drive with the Colab notebook to access your data directly from GDrive."]},{"cell_type":"code","metadata":{"id":"ZtpU68R4RxdT"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eK10D3MSpYty"},"source":["## Install enviroment"]},{"cell_type":"code","metadata":{"id":"asqMueYPeIgK"},"source":["!pip3 install urllib3==1.25.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPqtVgbkeTx7"},"source":["!pip3 install transformers==2.8.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mpkjTWefecLc"},"source":["!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/pretrain_transformers.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7zu3BqpqJQ7"},"source":["!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtAeHbhbTnzO"},"source":["%%writefile setup.sh\n","\n","export CUDA_HOME=/usr/local/cuda-10.1\n","git clone https://github.com/NVIDIA/apex\n","pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"udZ7AiMWTpD9"},"source":["!sh setup.sh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DP7YAlDPqknI"},"source":["## Add data to colab\n"]},{"cell_type":"code","metadata":{"id":"HddOX2tmKOoW"},"source":["# this is a dataset that was provided by Sberbank\n","# !wget https://raw.githubusercontent.com/FurkanArslan/twitter-text-classification/master/twitter_sentiment_corpus.csv\n","# data_path = \"twitter_sentiment_corpus.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mx9-md7OTE6U"},"source":["# here your data goes \n","# previously I removed all usernames starting with @ and loaded my Twitter data to my GDrive disk\n","# The format is a simple CSV/TSV table\n","\n","data_path = \"/content/gdrive/My Drive/MyTwitter/my_twitter_11_2020.tsv\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDYi1TVTrtkO"},"source":["## Prepare data"]},{"cell_type":"code","metadata":{"id":"OXdNbrq3rgzq"},"source":["import pandas as pd\n","\n","data = pd.read_csv(data_path, sep='\\t')\n","\n","# replace nans with a blank space\n","data = data.fillna(' ') \n","\n","print('Total tweets: ', len(data))\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HCnpgTL7VZBz"},"source":["# remove all empty tweets\n","data = data[data['tweet_clean'] != ' ']\n","print('Tweets after filtering: ', len(data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Izc0lkkHr2Rz"},"source":["import numpy as np\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tkc1_49r36X"},"source":["random.seed(1234)\n","np.random.seed(1234)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-IWaigRvyxmT"},"source":["data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WsC7iXoNy1eY"},"source":["# select 1000 random items for validation set\n","val_ind = random.sample(range(data.shape[0]), 1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0nbtCxLzTfM"},"source":["train = [data.iloc[i]['tweet_clean'] for i in range(len(data)) if i not in val_ind][:10000]\n","valid = [data.iloc[i]['tweet_clean'] for i in range(len(data)) if i in val_ind]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2T0gN6gqr9pa"},"source":["len(train), len(valid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPB8rrVPr-kh"},"source":["# save the datasets to the disk\n","with open(\"train.txt\", \"w\") as file:\n","    file.write(\"\\n\".join(train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HP5_nk_0sAB0"},"source":["with open(\"valid.txt\", \"w\") as file:\n","    file.write(\"\\n\".join(valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NitGcEKPsDQE"},"source":["## Run finetuning\n","The following code download our model and tokenizer from transformers and finetune model essays.\n","\n","This took aroung 15 minutes to train the model on 10 epochs. "]},{"cell_type":"code","metadata":{"id":"5vL07XFvsBBU"},"source":["!python pretrain_transformers.py \\\n","    --output_dir=comment_model \\\n","    --model_type=gpt2 \\\n","    --model_name_or_path=sberbank-ai/rugpt3small_based_on_gpt2 \\\n","    --do_train \\\n","    --train_data_file=train.txt \\\n","    --do_eval \\\n","    --fp16 \\\n","    --eval_data_file=valid.txt \\\n","    --per_gpu_train_batch_size 1 \\\n","    --gradient_accumulation_steps 1 \\\n","    --num_train_epochs 10 \\\n","    --block_size 2048 \\\n","    --overwrite_output_dir"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w2N6ylGPt1F5"},"source":["## Check our model"]},{"cell_type":"markdown","metadata":{"id":"gQWY_vFriLoF"},"source":["Here I added a script from this Notebook made by unknown Habr commentator: \n","https://colab.research.google.com/drive/1r5ufZF9SZPowAs0K8pQzESIjbcb1WTMd\n","\n","It's a curious stuff as it was made to generate horoscopes."]},{"cell_type":"code","metadata":{"id":"YcNZfjr5oguy"},"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"comment_model\") # here goes the folder name where our new model is saved\n","model = GPT2LMHeadModel.from_pretrained(\"comment_model\")\n","model.to(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"niRtjX_0oVZG"},"source":["import copy\n","\n","# to make the generated texts better, we remove \"bad\" tokens from the model, some junk symbols such as html tags, links etc. \n","\n","bad_word_ids = [\n","    [203], # \\n\n","    [225], # weird space 1\n","    [28664], # weird space 2\n","    [13298], # weird space 3\n","    [206], # \\r\n","    [49120], # html\n","    [25872], # http\n","    [3886], # amp\n","    [38512], # nbsp\n","    [10], # &\n","    [5436], # & (another)\n","    [5861], # http\n","    [372], # yet another line break\n","    [421, 4395], # МСК\n","    [64], # \\\n","    [33077], # https\n","    [1572], # ru\n","    [11101], # Источник\n","]\n","\n","def gen_fragment(context, bad_word_ids=bad_word_ids, print_debug_output=False, temperature=1.0, max_length=75, min_length=50):\n","    input_ids = tokenizer.encode(context, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n","    input_ids = input_ids[:, -1700:]\n","    input_size = input_ids.size(1)\n","    output_sequences = model.generate(\n","        input_ids=input_ids,\n","        max_length=max_length + input_size,\n","        min_length=min_length + input_size,\n","        top_p=0.95,\n","        do_sample=True,\n","        num_return_sequences=1,\n","        temperature=1.0,\n","        pad_token_id=0,\n","        eos_token_id=2,\n","        bad_words_ids=bad_word_ids,\n","        no_repeat_ngram_size=6\n","    )\n","    if len(output_sequences.shape) > 2:\n","        output_sequences.squeeze_()\n","    generated_sequence = output_sequences[0].tolist()[input_size:]\n","    if print_debug_output:\n","        for idx in generated_sequence:\n","            print(idx, tokenizer.decode([idx], clean_up_tokenization_spaces=True).strip())\n","    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n","    text = text[: text.find(\"</s>\")]\n","    text = text[: text.rfind(\".\") + 1]\n","    return context + text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfDcbrNCrISi"},"source":["beginning = \"Я люблю такие\"\n","print(gen_fragment(beginning, temperature=1.0, max_length=40))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xxjfpa7Mk9AI"},"source":["This is the original Sberbank's funcion to generate texts. I think it's output has too many artefacts but still good. \n"]},{"cell_type":"code","metadata":{"id":"JRlAAsIbsHdf"},"source":["!python generate_transformers.py \\\n","    --model_type=gpt2 \\\n","    --model_name_or_path=comment_model \\\n","    --k=0 \\\n","    --p=1.0 \\\n","    --length=40"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vzn-sWlp6vYt"},"source":["## Some examples I got from the gen_fragment function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G6FrdJGfoOCo"},"source":["Context >>> В Мадриде есть\n","\n","ruGPT:\n","В Мадриде есть  целый квартал, где сосредоточено все самое элитное. В старом городе много церквей и соборов. Но когда приезжаешь в Сан-Себастьян и видишь Мадрид как бы со стороны, то кажется, что ты на вершине горы.\n"]},{"cell_type":"markdown","metadata":{"id":"9xrK05vCoUXh"},"source":["Context >>> Вчера вечером я\n","\n","ruGPT:\n","Вчера вечером я  видела, как два чувака лет 30 сидели тихо в кафе в парке, пили пиво и иногда ругались матом. В общем, весь вечер гуляли, никого не трогали.\n"]},{"cell_type":"markdown","metadata":{"id":"aOo1x1ArogLs"},"source":["Context >>> Моя любимая книга\n","\n","ruGPT:\n","Моя любимая книга о жизни :) Когда читаешь ее с таким восторженным взглядом, даже не знаешь, что там на самом деле. Книга очень клевая. Только герои не очень. Особенно Вуди Аллен какой-то скучный.\n"]},{"cell_type":"markdown","metadata":{"id":"mZ1g2FbUl9Z9"},"source":["Context >>> Мне нравится\n","\n","ruGPT: Мне нравится этот актер. Почему? Он хороший, умный и добрый.\n"]},{"cell_type":"markdown","metadata":{"id":"D34DPrwvozgd"},"source":["## Some examples from Sberbank function"]},{"cell_type":"markdown","metadata":{"id":"FQGx18PAm7uH"},"source":["Context >>> В Мадриде есть\n","\n","ruGPT В Мадриде есть так называемые Tower Rules Officers - это массовые рейды вдоль побережья, каждые выходные. Но помимо всего, толпы народа, адово пыль.\n"]},{"cell_type":"markdown","metadata":{"id":"_gAHd3q5nDBR"},"source":["Context >>> Вчера вечером я \n","\n","ruGPT: Вчера вечером я гуляла по лесу и наткнулась на мужика лет 40-50 с косичками. Фотографироваться не стала, пошла дальше.\n"]},{"cell_type":"markdown","metadata":{"id":"b4qPDAZgnfLx"},"source":["Context >>> Моя любимая книга \n","\n","ruGPT: Моя любимая книга у моего учителя русского. Ребенок провел за написание одной страницы в месяц 8 часов, 4 страницы над каждой. Итог — 6 книг из 6,5."]},{"cell_type":"markdown","metadata":{"id":"4RT0yL5NoTOv"},"source":["Context >>> Мне нравится\n","\n","ruGPT: Мне нравится Тимур Родригес. И песня запоминается надолго."]}]}